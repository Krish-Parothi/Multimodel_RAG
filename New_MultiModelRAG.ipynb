{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69261b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Placement Preparations\\LANGCHAIN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import fitz #PyMuPDF ke andar ye use hota hai.\n",
    "import base64\n",
    "import torch \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.schema.messages import HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569a3d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading Clip Model so we reuired processor and model\n",
    "load_dotenv()\n",
    "\n",
    "## set up the environment\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Initialize Clip Model\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# This model is reponsible for conversion of text and images into embeddings.\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# CLIP processor isliye use hota hai kyunki CLIP model ko image aur text ek specific standardized format me chahiye hota hai â€” processor unhe convert karke ready-to-use banata hai.\n",
    "\n",
    "clip_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5e71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e6d7d8",
   "metadata": {},
   "source": [
    "## Embeddings of Image and Text Using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26cabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image_data):\n",
    "    ''' Embbed image using clip'''\n",
    "    if isinstance(image_data, str): # if path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else: # If PIL Image\n",
    "        image = image_data\n",
    "\n",
    "    input = clip_processor(images=image, return_tensors=\"pt\") # we need to return tensors in pytorch tensors.\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**input)\n",
    "        # Normalize embeddings to unit vectors\n",
    "        features = features/features.norm(dim=1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "    \n",
    "def embed_text(text):\n",
    "    ''' Embed text using CLIP'''\n",
    "    inputs = clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77 # Clip's Max token length\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features/features.norm(dim=1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784df37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path = \"multimodal_sample.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# we now create variables for Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = []\n",
    "\n",
    "# Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500) \n",
    "# RecursiveCharacterTextSplitter large text ko intelligent, meaning-preserving chunks me todta hai taaki embeddings and RAG best perform karein.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c2b53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('Practical.pdf')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99851e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 0 on page 1: list indices must be integers or slices, not str\n",
      "Error processing image 0 on page 2: list indices must be integers or slices, not str\n",
      "Error processing image 1 on page 2: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "for i,page in enumerate(doc): #go inside my doc\n",
    "    # Proess the text\n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        # create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        # For all the text data, keep meta data type as text only.\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399d38ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='PRACTICAL NO : 4  \\nNAME : KRISH PAROTHI  \\nSECTION: A4 \\nBATCH: B3  \\nROLL NO. : 49  \\nSUBJECT : COMPUTER NETWORKS \\n  \\nAIM : To Implement Data link Layer flow control mechanism Develop a simple data link \\nlayer that performs the flow control using the sliding window protocol, and loss recovery \\nusing the Go-Back-N mechanism:-   \\nA small file (set of data packets) needs to be transmitted over a channel that may lose or'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='using the Go-Back-N mechanism:-   \\nA small file (set of data packets) needs to be transmitted over a channel that may lose or \\nreorder packets. To manage the flow and recover from lost packets, the sender and receiver \\nimplement the sliding window protocol with Go-Back-N (window size = 4). If a packet \\nacknowledgment is not received within a fixed timeout, the sender should retransmit all \\npackets from the unacknowledged one.  \\n  \\nCODE :  \\nimport random  \\n  \\nTOTAL_PACKETS = 10'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='acknowledgment is not received within a fixed timeout, the sender should retransmit all \\npackets from the unacknowledged one.  \\n  \\nCODE :  \\nimport random  \\n  \\nTOTAL_PACKETS = 10  \\nWINDOW_SIZE = 4  \\nLOSS_PROBABILITY = 0.2  # 20% chance a packet is lost  \\n  \\ndef send_packets():  \\n    base = 0  \\n    while base < TOTAL_PACKETS:  \\n        print(f\"\\\\nWindow: Sending packets {base} to {min(base + \\nWINDOW_SIZE - 1,  \\nTOTAL_PACKETS - 1)}\")  \\n        acked = True'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='base = 0  \\n    while base < TOTAL_PACKETS:  \\n        print(f\"\\\\nWindow: Sending packets {base} to {min(base + \\nWINDOW_SIZE - 1,  \\nTOTAL_PACKETS - 1)}\")  \\n        acked = True  \\n  \\n        # Try sending packets in window  \\n        for i in range(base, min(base + WINDOW_SIZE, TOTAL_PACKETS)):  \\n            if random.random() < LOSS_PROBABILITY:  \\n                print(f\"Packet {i} LOST\")  \\n                acked = False  \\n                break  # Go-Back-N: stop and resend from here'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='if random.random() < LOSS_PROBABILITY:  \\n                print(f\"Packet {i} LOST\")  \\n                acked = False  \\n                break  # Go-Back-N: stop and resend from here  \\n            else:  \\n                print(f\"Packet {i} sent successfully\")  \\n  \\n        if acked:  \\n            base += WINDOW_SIZE  # All packets acked, slide window  \\n        else:  \\n            print(\"Go-Back-N: Resending from lost packet...\")'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='send_packets() \\n \\n \\nCODE SCREENSHOT:'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='OUTPUT :')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89c5efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0177418 ,  0.01462063, -0.00422945, ...,  0.0441241 ,\n",
       "         0.00833613, -0.00232817],\n",
       "       [ 0.02780538,  0.01399401,  0.00153549, ...,  0.00678344,\n",
       "         0.01409649,  0.04111386],\n",
       "       [ 0.01202888,  0.01678002, -0.03601726, ..., -0.06275425,\n",
       "        -0.03378645,  0.05293867],\n",
       "       ...,\n",
       "       [ 0.02796599,  0.0054474 , -0.01877345, ..., -0.08481495,\n",
       "        -0.01538367,  0.0349141 ],\n",
       "       [-0.00024179, -0.01857516, -0.03511811, ...,  0.02345825,\n",
       "        -0.03515678,  0.03541576],\n",
       "       [ 0.00902395,  0.01864137, -0.0012215 , ..., -0.00430229,\n",
       "         0.00424004,  0.01835299]], shape=(7, 512), dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create unified FAISS vector store with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22617844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'page': 0, 'type': 'text'}, page_content='PRACTICAL NO : 4  \\nNAME : KRISH PAROTHI  \\nSECTION: A4 \\nBATCH: B3  \\nROLL NO. : 49  \\nSUBJECT : COMPUTER NETWORKS \\n  \\nAIM : To Implement Data link Layer flow control mechanism Develop a simple data link \\nlayer that performs the flow control using the sliding window protocol, and loss recovery \\nusing the Go-Back-N mechanism:-   \\nA small file (set of data packets) needs to be transmitted over a channel that may lose or'),\n",
       "  Document(metadata={'page': 0, 'type': 'text'}, page_content='using the Go-Back-N mechanism:-   \\nA small file (set of data packets) needs to be transmitted over a channel that may lose or \\nreorder packets. To manage the flow and recover from lost packets, the sender and receiver \\nimplement the sliding window protocol with Go-Back-N (window size = 4). If a packet \\nacknowledgment is not received within a fixed timeout, the sender should retransmit all \\npackets from the unacknowledged one.  \\n  \\nCODE :  \\nimport random  \\n  \\nTOTAL_PACKETS = 10'),\n",
       "  Document(metadata={'page': 0, 'type': 'text'}, page_content='acknowledgment is not received within a fixed timeout, the sender should retransmit all \\npackets from the unacknowledged one.  \\n  \\nCODE :  \\nimport random  \\n  \\nTOTAL_PACKETS = 10  \\nWINDOW_SIZE = 4  \\nLOSS_PROBABILITY = 0.2  # 20% chance a packet is lost  \\n  \\ndef send_packets():  \\n    base = 0  \\n    while base < TOTAL_PACKETS:  \\n        print(f\"\\\\nWindow: Sending packets {base} to {min(base + \\nWINDOW_SIZE - 1,  \\nTOTAL_PACKETS - 1)}\")  \\n        acked = True'),\n",
       "  Document(metadata={'page': 0, 'type': 'text'}, page_content='base = 0  \\n    while base < TOTAL_PACKETS:  \\n        print(f\"\\\\nWindow: Sending packets {base} to {min(base + \\nWINDOW_SIZE - 1,  \\nTOTAL_PACKETS - 1)}\")  \\n        acked = True  \\n  \\n        # Try sending packets in window  \\n        for i in range(base, min(base + WINDOW_SIZE, TOTAL_PACKETS)):  \\n            if random.random() < LOSS_PROBABILITY:  \\n                print(f\"Packet {i} LOST\")  \\n                acked = False  \\n                break  # Go-Back-N: stop and resend from here'),\n",
       "  Document(metadata={'page': 0, 'type': 'text'}, page_content='if random.random() < LOSS_PROBABILITY:  \\n                print(f\"Packet {i} LOST\")  \\n                acked = False  \\n                break  # Go-Back-N: stop and resend from here  \\n            else:  \\n                print(f\"Packet {i} sent successfully\")  \\n  \\n        if acked:  \\n            base += WINDOW_SIZE  # All packets acked, slide window  \\n        else:  \\n            print(\"Go-Back-N: Resending from lost packet...\")'),\n",
       "  Document(metadata={'page': 1, 'type': 'text'}, page_content='send_packets() \\n \\n \\nCODE SCREENSHOT:'),\n",
       "  Document(metadata={'page': 2, 'type': 'text'}, page_content='OUTPUT :')],\n",
       " array([[ 0.0177418 ,  0.01462063, -0.00422945, ...,  0.0441241 ,\n",
       "          0.00833613, -0.00232817],\n",
       "        [ 0.02780538,  0.01399401,  0.00153549, ...,  0.00678344,\n",
       "          0.01409649,  0.04111386],\n",
       "        [ 0.01202888,  0.01678002, -0.03601726, ..., -0.06275425,\n",
       "         -0.03378645,  0.05293867],\n",
       "        ...,\n",
       "        [ 0.02796599,  0.0054474 , -0.01877345, ..., -0.08481495,\n",
       "         -0.01538367,  0.0349141 ],\n",
       "        [-0.00024179, -0.01857516, -0.03511811, ...,  0.02345825,\n",
       "         -0.03515678,  0.03541576],\n",
       "        [ 0.00902395,  0.01864137, -0.0012215 , ..., -0.00430229,\n",
       "          0.00424004,  0.01835299]], shape=(7, 512), dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(all_docs,embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70956898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1e06ae5ae40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fad0688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E0FA6F01A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E0FA6F0EC0>, model_name='meta-llama/llama-4-scout-17b-16e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Groq: Llama Vision model\n",
    "llm = init_chat_model(\"groq:meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b98c5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4dbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80498b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e5b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Explain first 10 pages of pdf\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 2: OUTPUT :\n",
      "  - Text from page 1: send_packets() \n",
      " \n",
      " \n",
      "CODE SCREENSHOT:\n",
      "  - Text from page 0: if random.random() < LOSS_PROBABILITY:  \n",
      "                print(f\"Packet {i} LOST\")  \n",
      "               ...\n",
      "  - Text from page 0: acknowledgment is not received within a fixed timeout, the sender should retransmit all \n",
      "packets fro...\n",
      "  - Text from page 0: base = 0  \n",
      "    while base < TOTAL_PACKETS:  \n",
      "        print(f\"\\nWindow: Sending packets {base} to {mi...\n",
      "\n",
      "\n",
      "Answer: Based on the provided text excerpts, I'll explain the first 10 pages of the PDF, which seems to be a code snippet and explanation of the Go-Back-N protocol.\n",
      "\n",
      "**Pages 0-1:**\n",
      "The code snippet is implementing the Go-Back-N protocol, a transport-layer protocol used for reliable data transfer over an unreliable network. The protocol is used to ensure that data packets are delivered in the correct order.\n",
      "\n",
      "The code initializes variables:\n",
      "\n",
      "* `TOTAL_PACKETS`: The total number of packets to be sent (set to 10).\n",
      "* `WINDOW_SIZE`: The size of the sender's window (set to 4).\n",
      "* `LOSS_PROBABILITY`: The probability of a packet being lost (set to 0.2 or 20%).\n",
      "\n",
      "The `send_packets()` function is defined, which implements the Go-Back-N protocol.\n",
      "\n",
      "**Page 0-2:**\n",
      "The `send_packets()` function works as follows:\n",
      "\n",
      "1. It initializes a `base` variable to 0, which represents the starting point of the sender's window.\n",
      "2. It enters a while loop that continues until all packets have been sent (i.e., `base < TOTAL_PACKETS`).\n",
      "3. Within the loop, it prints the current window of packets being sent (from `base` to `min(base + WINDOW_SIZE - 1, TOTAL_PACKETS - 1)`).\n",
      "4. It assumes that all packets in the window are acknowledged (i.e., `acked = True`).\n",
      "5. It then tries sending packets in the current window using a for loop.\n",
      "\n",
      "**Page 2:**\n",
      "The output of the code is shown, which indicates the packets being sent and any packets that are lost.\n",
      "\n",
      "**Page 3-10 (not shown):**\n",
      "Although the text excerpts do not provide information about the next pages, based on the code snippet and explanation provided, it can be inferred that the next pages likely continue explaining the Go-Back-N protocol, possibly covering:\n",
      "\n",
      "* How the sender handles packet loss and retransmission\n",
      "* How the receiver sends acknowledgments\n",
      "* Examples of the protocol in action\n",
      "* Discussion of the advantages and disadvantages of the Go-Back-N protocol\n",
      "\n",
      "However, without the actual text excerpts from pages 3-10, it's difficult to provide a more detailed explanation.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \n",
    "        \"Explain first 10 pages of pdf\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fcdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd2dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b092ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e6cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
